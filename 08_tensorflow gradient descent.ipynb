{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fun(w, x, y):\n",
    "    hypothesis = x * w\n",
    "    return tf.reduce_mean(tf.square(hypothesis - y))\n",
    "\n",
    "# tf.reduce_mean(x, axis)\n",
    "# x에는 행렬이 들어간다 np.array\n",
    "# axis=None 일 때, 전체의 평균\n",
    "# axis=0 일 때, 각 열의 평균\n",
    "# axis=1 일 때, 각 행의 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "for feed_w in np.linspace(-3, 5, num=15):\n",
    "    curr_cost = cost_fun(feed_w, X, Y)\n",
    "    print(\"{:6.3f} | {:10.5f}\".format(feed_w, curr_cost))\n",
    "\n",
    "# np.linspace(a, b, between)\n",
    "# a와 b 사이의 실수가 동일 간격으로 between개 만큼 담긴 열벡터를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal((1,), -100., 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.random.normal(\n",
    "    shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None, name=None\n",
    ")`\n",
    "\n",
    "* `shape` : [행의 갯수, 열의 갯수] 의 배열형태로 대입, 텐서가 증가하면 더 앞으로 숫자를 추가한다.\n",
    "* `mean` : 평균\n",
    "* `stddev` : 표준편차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Variable(\n",
    "    initial_value=None, trainable=None, validate_shape=True, caching_device=None,\n",
    "    name=None, variable_def=None, dtype=None, import_scope=None, constraint=None,\n",
    "    synchronization=tf.VariableSynchronization.AUTO,\n",
    "    aggregation=tf.compat.v1.VariableAggregation.NONE, shape=None\n",
    ")`\n",
    "* `initial_value` : 초기값, 행과 열 잘 판단해서 대입\n",
    "* `.assign(a)` : 값을 a로 대체\n",
    "* `.assign_add(a)` : 값에 a를 더함\n",
    "* `.assign_sub(a)` : 값에 a를 뺌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 11716.3086 |  48.767971\n",
      "   10 |  4504.9126 |  30.619968\n",
      "   20 |  1732.1364 |  19.366755\n",
      "   30 |   666.0052 |  12.388859\n",
      "   40 |   256.0785 |   8.062004\n",
      "   50 |    98.4620 |   5.379007\n",
      "   60 |    37.8586 |   3.715335\n",
      "   70 |    14.5566 |   2.683725\n",
      "   80 |     5.5970 |   2.044044\n",
      "   90 |     2.1520 |   1.647391\n",
      "  100 |     0.8275 |   1.401434\n",
      "  110 |     0.3182 |   1.248922\n",
      "  120 |     0.1223 |   1.154351\n",
      "  130 |     0.0470 |   1.095710\n",
      "  140 |     0.0181 |   1.059348\n",
      "  150 |     0.0070 |   1.036801\n",
      "  160 |     0.0027 |   1.022819\n",
      "  170 |     0.0010 |   1.014150\n",
      "  180 |     0.0004 |   1.008774\n",
      "  190 |     0.0002 |   1.005441\n",
      "  200 |     0.0001 |   1.003374\n",
      "  210 |     0.0000 |   1.002092\n",
      "  220 |     0.0000 |   1.001297\n",
      "  230 |     0.0000 |   1.000804\n",
      "  240 |     0.0000 |   1.000499\n",
      "  250 |     0.0000 |   1.000309\n",
      "  260 |     0.0000 |   1.000192\n",
      "  270 |     0.0000 |   1.000119\n",
      "  280 |     0.0000 |   1.000074\n",
      "  290 |     0.0000 |   1.000046\n"
     ]
    }
   ],
   "source": [
    "for step in range(300):\n",
    "    hypothesis = W*X\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    \n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X)-Y, X))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign(descent)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5.0001483], shape=(1,), dtype=float32)\n",
      "tf.Tensor([2.5000741], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(5.0 * W)\n",
    "print(2.5 * W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   914.6667 |  14.346666\n",
      "   10 |   351.6888 |   9.276003\n",
      "   20 |   135.2241 |   6.131784\n",
      "   30 |    51.9936 |   4.182118\n",
      "   40 |    19.9915 |   2.973168\n",
      "   50 |     7.6867 |   2.223522\n",
      "   60 |     2.9555 |   1.758682\n",
      "   70 |     1.1364 |   1.470443\n",
      "   80 |     0.4369 |   1.291713\n",
      "   90 |     0.1680 |   1.180885\n",
      "  100 |     0.0646 |   1.112163\n",
      "  110 |     0.0248 |   1.069550\n",
      "  120 |     0.0096 |   1.043127\n",
      "  130 |     0.0037 |   1.026742\n",
      "  140 |     0.0014 |   1.016582\n",
      "  150 |     0.0005 |   1.010282\n",
      "  160 |     0.0002 |   1.006376\n",
      "  170 |     0.0001 |   1.003954\n",
      "  180 |     0.0000 |   1.002451\n",
      "  190 |     0.0000 |   1.001520\n",
      "  200 |     0.0000 |   1.000942\n",
      "  210 |     0.0000 |   1.000584\n",
      "  220 |     0.0000 |   1.000362\n",
      "  230 |     0.0000 |   1.000225\n",
      "  240 |     0.0000 |   1.000139\n",
      "  250 |     0.0000 |   1.000086\n",
      "  260 |     0.0000 |   1.000054\n",
      "  270 |     0.0000 |   1.000033\n",
      "  280 |     0.0000 |   1.000021\n",
      "  290 |     0.0000 |   1.000013\n"
     ]
    }
   ],
   "source": [
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]\n",
    "\n",
    "W = tf.Variable([15.0])\n",
    "\n",
    "for step in range(300):\n",
    "    hypothesis = W * X\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    \n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X)-Y, X))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign(descent)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-variable linear regression\n",
    "\n",
    "x1 = [73., 93., 89., 96., 73.]\n",
    "x2 = [80., 83., 91., 98., 66.]\n",
    "x3 = [75., 93., 90., 100., 70.]\n",
    "y  = [152., 185., 180., 196., 142.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, w3, b = tf.Variable(1.), tf.Variable(1.), tf.Variable(1.), tf.Variable(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |    7172.0000\n",
      "   10 |    2940.2720\n",
      "   20 |    1207.2152\n",
      "   30 |     497.4604\n",
      "   40 |     206.7852\n",
      "   50 |      87.7412\n",
      "   60 |      38.9854\n",
      "   70 |      19.0159\n",
      "   80 |      10.8354\n",
      "   90 |       7.4829\n",
      "  100 |       6.1076\n",
      "  110 |       5.5421\n",
      "  120 |       5.3082\n",
      "  130 |       5.2102\n",
      "  140 |       5.1678\n",
      "  150 |       5.1482\n",
      "  160 |       5.1379\n",
      "  170 |       5.1314\n",
      "  180 |       5.1265\n",
      "  190 |       5.1223\n",
      "  200 |       5.1183\n",
      "  210 |       5.1144\n",
      "  220 |       5.1106\n",
      "  230 |       5.1068\n",
      "  240 |       5.1030\n",
      "  250 |       5.0992\n",
      "  260 |       5.0954\n",
      "  270 |       5.0916\n",
      "  280 |       5.0878\n",
      "  290 |       5.0840\n",
      "  300 |       5.0803\n",
      "  310 |       5.0765\n",
      "  320 |       5.0727\n",
      "  330 |       5.0690\n",
      "  340 |       5.0652\n",
      "  350 |       5.0614\n",
      "  360 |       5.0577\n",
      "  370 |       5.0539\n",
      "  380 |       5.0502\n",
      "  390 |       5.0465\n",
      "  400 |       5.0427\n",
      "  410 |       5.0390\n",
      "  420 |       5.0352\n",
      "  430 |       5.0315\n",
      "  440 |       5.0278\n",
      "  450 |       5.0241\n",
      "  460 |       5.0203\n",
      "  470 |       5.0166\n",
      "  480 |       5.0129\n",
      "  490 |       5.0092\n",
      "  500 |       5.0055\n",
      "  510 |       5.0018\n",
      "  520 |       4.9981\n",
      "  530 |       4.9944\n",
      "  540 |       4.9907\n",
      "  550 |       4.9870\n",
      "  560 |       4.9833\n",
      "  570 |       4.9796\n",
      "  580 |       4.9759\n",
      "  590 |       4.9723\n",
      "  600 |       4.9686\n",
      "  610 |       4.9649\n",
      "  620 |       4.9612\n",
      "  630 |       4.9576\n",
      "  640 |       4.9539\n",
      "  650 |       4.9503\n",
      "  660 |       4.9466\n",
      "  670 |       4.9429\n",
      "  680 |       4.9393\n",
      "  690 |       4.9356\n",
      "  700 |       4.9320\n",
      "  710 |       4.9283\n",
      "  720 |       4.9247\n",
      "  730 |       4.9210\n",
      "  740 |       4.9174\n",
      "  750 |       4.9138\n",
      "  760 |       4.9102\n",
      "  770 |       4.9065\n",
      "  780 |       4.9029\n",
      "  790 |       4.8993\n",
      "  800 |       4.8957\n",
      "  810 |       4.8921\n",
      "  820 |       4.8885\n",
      "  830 |       4.8849\n",
      "  840 |       4.8813\n",
      "  850 |       4.8777\n",
      "  860 |       4.8741\n",
      "  870 |       4.8705\n",
      "  880 |       4.8669\n",
      "  890 |       4.8633\n",
      "  900 |       4.8597\n",
      "  910 |       4.8562\n",
      "  920 |       4.8526\n",
      "  930 |       4.8490\n",
      "  940 |       4.8455\n",
      "  950 |       4.8419\n",
      "  960 |       4.8383\n",
      "  970 |       4.8348\n",
      "  980 |       4.8312\n",
      "  990 |       4.8276\n",
      " 1000 |       4.8241\n"
     ]
    }
   ],
   "source": [
    "for i in range(1001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis-y))\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('{:5} | {:12.4f}'.format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using matrix\n",
    "data = np.array([\n",
    "    [73., 80., 75., 152.],\n",
    "    [93., 83., 93., 185.],\n",
    "    [89., 91., 90., 180.],\n",
    "    [73., 66., 70., 142.],\n",
    "], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73.,  80.,  75., 152.],\n",
       "       [ 93.,  83.,  93., 185.],\n",
       "       [ 89.,  91.,  90., 180.],\n",
       "       [ 73.,  66.,  70., 142.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 | 107070.7500\n",
      "   10 | 47173.6250\n",
      "   20 | 20788.2832\n",
      "   30 |  9165.2402\n",
      "   40 |  4045.1636\n",
      "   50 |  1789.7024\n",
      "   60 |   796.1393\n",
      "   70 |   358.4574\n",
      "   80 |   165.6460\n",
      "   90 |    80.7029\n",
      "  100 |    43.2774\n",
      "  110 |    26.7833\n",
      "  120 |    19.5099\n",
      "  130 |    16.2983\n",
      "  140 |    14.8760\n",
      "  150 |    14.2419\n",
      "  160 |    13.9551\n",
      "  170 |    13.8212\n",
      "  180 |    13.7547\n",
      "  190 |    13.7180\n",
      "  200 |    13.6942\n",
      "  210 |    13.6763\n",
      "  220 |    13.6608\n",
      "  230 |    13.6466\n",
      "  240 |    13.6329\n",
      "  250 |    13.6193\n",
      "  260 |    13.6059\n",
      "  270 |    13.5925\n",
      "  280 |    13.5793\n",
      "  290 |    13.5660\n",
      "  300 |    13.5526\n",
      "  310 |    13.5394\n",
      "  320 |    13.5261\n",
      "  330 |    13.5129\n",
      "  340 |    13.4997\n",
      "  350 |    13.4864\n",
      "  360 |    13.4732\n",
      "  370 |    13.4600\n",
      "  380 |    13.4469\n",
      "  390 |    13.4337\n",
      "  400 |    13.4206\n",
      "  410 |    13.4075\n",
      "  420 |    13.3944\n",
      "  430 |    13.3813\n",
      "  440 |    13.3682\n",
      "  450 |    13.3551\n",
      "  460 |    13.3420\n",
      "  470 |    13.3290\n",
      "  480 |    13.3160\n",
      "  490 |    13.3029\n",
      "  500 |    13.2899\n",
      "  510 |    13.2769\n",
      "  520 |    13.2639\n",
      "  530 |    13.2509\n",
      "  540 |    13.2380\n",
      "  550 |    13.2250\n",
      "  560 |    13.2121\n",
      "  570 |    13.1992\n",
      "  580 |    13.1863\n",
      "  590 |    13.1734\n",
      "  600 |    13.1605\n",
      "  610 |    13.1477\n",
      "  620 |    13.1348\n",
      "  630 |    13.1220\n",
      "  640 |    13.1092\n",
      "  650 |    13.0964\n",
      "  660 |    13.0836\n",
      "  670 |    13.0708\n",
      "  680 |    13.0580\n",
      "  690 |    13.0453\n",
      "  700 |    13.0325\n",
      "  710 |    13.0198\n",
      "  720 |    13.0071\n",
      "  730 |    12.9945\n",
      "  740 |    12.9817\n",
      "  750 |    12.9691\n",
      "  760 |    12.9564\n",
      "  770 |    12.9437\n",
      "  780 |    12.9311\n",
      "  790 |    12.9185\n",
      "  800 |    12.9059\n",
      "  810 |    12.8932\n",
      "  820 |    12.8806\n",
      "  830 |    12.8681\n",
      "  840 |    12.8555\n",
      "  850 |    12.8430\n",
      "  860 |    12.8305\n",
      "  870 |    12.8179\n",
      "  880 |    12.8055\n",
      "  890 |    12.7929\n",
      "  900 |    12.7805\n",
      "  910 |    12.7680\n",
      "  920 |    12.7555\n",
      "  930 |    12.7431\n",
      "  940 |    12.7307\n",
      "  950 |    12.7183\n",
      "  960 |    12.7059\n",
      "  970 |    12.6935\n",
      "  980 |    12.6811\n",
      "  990 |    12.6688\n",
      " 1000 |    12.6564\n",
      " 1010 |    12.6440\n",
      " 1020 |    12.6317\n",
      " 1030 |    12.6194\n",
      " 1040 |    12.6071\n",
      " 1050 |    12.5948\n",
      " 1060 |    12.5825\n",
      " 1070 |    12.5702\n",
      " 1080 |    12.5580\n",
      " 1090 |    12.5458\n",
      " 1100 |    12.5335\n",
      " 1110 |    12.5213\n",
      " 1120 |    12.5092\n",
      " 1130 |    12.4970\n",
      " 1140 |    12.4848\n",
      " 1150 |    12.4727\n",
      " 1160 |    12.4605\n",
      " 1170 |    12.4483\n",
      " 1180 |    12.4362\n",
      " 1190 |    12.4241\n",
      " 1200 |    12.4120\n",
      " 1210 |    12.4000\n",
      " 1220 |    12.3879\n",
      " 1230 |    12.3758\n",
      " 1240 |    12.3638\n",
      " 1250 |    12.3517\n",
      " 1260 |    12.3397\n",
      " 1270 |    12.3277\n",
      " 1280 |    12.3157\n",
      " 1290 |    12.3037\n",
      " 1300 |    12.2918\n",
      " 1310 |    12.2798\n",
      " 1320 |    12.2678\n",
      " 1330 |    12.2559\n",
      " 1340 |    12.2440\n",
      " 1350 |    12.2321\n",
      " 1360 |    12.2201\n",
      " 1370 |    12.2083\n",
      " 1380 |    12.1964\n",
      " 1390 |    12.1846\n",
      " 1400 |    12.1727\n",
      " 1410 |    12.1609\n",
      " 1420 |    12.1490\n",
      " 1430 |    12.1373\n",
      " 1440 |    12.1255\n",
      " 1450 |    12.1136\n",
      " 1460 |    12.1019\n",
      " 1470 |    12.0901\n",
      " 1480 |    12.0783\n",
      " 1490 |    12.0666\n",
      " 1500 |    12.0549\n",
      " 1510 |    12.0432\n",
      " 1520 |    12.0314\n",
      " 1530 |    12.0198\n",
      " 1540 |    12.0081\n",
      " 1550 |    11.9964\n",
      " 1560 |    11.9847\n",
      " 1570 |    11.9731\n",
      " 1580 |    11.9615\n",
      " 1590 |    11.9499\n",
      " 1600 |    11.9382\n",
      " 1610 |    11.9267\n",
      " 1620 |    11.9151\n",
      " 1630 |    11.9035\n",
      " 1640 |    11.8920\n",
      " 1650 |    11.8804\n",
      " 1660 |    11.8689\n",
      " 1670 |    11.8574\n",
      " 1680 |    11.8459\n",
      " 1690 |    11.8343\n",
      " 1700 |    11.8228\n",
      " 1710 |    11.8114\n",
      " 1720 |    11.7999\n",
      " 1730 |    11.7885\n",
      " 1740 |    11.7770\n",
      " 1750 |    11.7656\n",
      " 1760 |    11.7542\n",
      " 1770 |    11.7428\n",
      " 1780 |    11.7314\n",
      " 1790 |    11.7200\n",
      " 1800 |    11.7087\n",
      " 1810 |    11.6973\n",
      " 1820 |    11.6859\n",
      " 1830 |    11.6746\n",
      " 1840 |    11.6632\n",
      " 1850 |    11.6519\n",
      " 1860 |    11.6406\n",
      " 1870 |    11.6294\n",
      " 1880 |    11.6181\n",
      " 1890 |    11.6068\n",
      " 1900 |    11.5956\n",
      " 1910 |    11.5844\n",
      " 1920 |    11.5731\n",
      " 1930 |    11.5619\n",
      " 1940 |    11.5507\n",
      " 1950 |    11.5395\n",
      " 1960 |    11.5283\n",
      " 1970 |    11.5172\n",
      " 1980 |    11.5060\n",
      " 1990 |    11.4948\n",
      " 2000 |    11.4837\n"
     ]
    }
   ],
   "source": [
    "X = data[:, :-1]\n",
    "Y = data[:, [-1]]\n",
    "\n",
    "W = tf.Variable([[1.], [2.], [3.]])\n",
    "b = tf.Variable([[2.]])\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean(tf.square(predict(X)-Y))\n",
    "        \n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i %10 == 0:\n",
    "        print('{:5} | {:10.4f}'.format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tf.square` : 제곱\n",
    "* `tf.matmul(X, Y)` : 행렬 X와 Y를 행렬곱으로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tf.GradientTape()` : 변화를 감지하고 저장\n",
    "* `.watch(a)` : 현재 확인하고 있는 tensor a를 지정한다.\n",
    "* `.gradient(target, source)` : d(source) / d(target) 을(이전 값과의 차이를 이용) 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
